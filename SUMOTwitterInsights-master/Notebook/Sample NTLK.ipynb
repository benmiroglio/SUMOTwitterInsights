{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Analyzer to identify postive and negative words in a Twitter CSV file with tag info and 280 characters of Tweets' Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Functions\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "########################################\n",
    "# Once you download the data\n",
    "# and change the paths below,\n",
    "# this script should run fine\n",
    "# runnning the following commands\n",
    "# in a terminal from the top level\n",
    "# directory.\n",
    "\n",
    "# First, ensure all dependencies are installed\n",
    "# pip install -r requirements.txt\n",
    "\n",
    "# Run the script\n",
    "# python3 sentiment/sentiment_example.py\n",
    "\n",
    "#######################################\n",
    "\n",
    "# csv file downloaded from\n",
    "# https://www.kaggle.com/kazanova/sentiment140\n",
    "# TRAINPATH should be changed to reflect your local\n",
    "# filepath\n",
    "TRAINPATH = \"training.1600000.processed.noemoticon.csv\"\n",
    "\n",
    "\n",
    "# csv file originally named \"Week 3 Social Firefox 63 Desktop - Sheet8.csv\"\n",
    "# TARGET_PATH should be changed to reflect your local filepath\n",
    "TARGETPATH = \"week3social.csv\"\n",
    "\n",
    "\n",
    "# ignore pesky pandas warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "# words to ignore i.e. \"the\", \"a\", ..., etc.\n",
    "STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "\n",
    "# map raw tweet labels\n",
    "# to human-readable labels\n",
    "SENTIMENT_MAP = {\n",
    "    0: \"neg\",\n",
    "    2: \"neut\",\n",
    "    4: \"pos\"\n",
    "}\n",
    "\n",
    "\n",
    "def tokenize_tweet(tweet):\n",
    "    # this will clean text, ignoring\n",
    "    # punctuation and splitting into\n",
    "    # individual words\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # text is the last element\n",
    "    # here we clean the text via the\n",
    "    # tokenizer defined above\n",
    "    text = tokenizer.tokenize(tweet)\n",
    "    return [i.lower() for i in text if i not in STOP_WORDS]\n",
    "\n",
    "\n",
    "def trim_data(data):\n",
    "    # map 0, 2, 4 to neg, neut, pos\n",
    "    # None for any non_training data \n",
    "    if \"label\" in data:\n",
    "        data.loc[:, \"label\"] = [SENTIMENT_MAP.get(x) for x in data[\"label\"]]\n",
    "    else:\n",
    "        data.loc[:, \"label\"] = None\n",
    "\n",
    "    # map each tweet to a list of non-stop words\n",
    "    data.loc[:, \"words\"] = [tokenize_tweet(tweet) for tweet in data['text']]\n",
    "\n",
    "    return data[[\"label\", \"words\"]]\n",
    "\n",
    "\n",
    "def read_train_data():\n",
    "    data = (\n",
    "        pd\n",
    "        .read_csv(TRAINPATH, encoding=\"ISO-8859-1\", header=None)\n",
    "        .sample(10000)  # take a sample for now\n",
    "    )\n",
    "\n",
    "    # label the columns\n",
    "    data.columns = [\"label\", \"id\", \"date\",\n",
    "                    \"flag\", \"user\", \"text\"]\n",
    "\n",
    "    return trim_data(data)\n",
    "\n",
    "\n",
    "def read_target_data():\n",
    "    data = pd.read_csv(TARGETPATH, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    # only english tweets\n",
    "    en = data[[\"EN\" in str(i) for i in data.tags]]\n",
    "    return trim_data(en)\n",
    "\n",
    "\n",
    "def get_top_words(df, top):\n",
    "    df.reset_index(inplace=True)\n",
    "    rows = []\n",
    "    _ = df.apply(lambda row: [rows.append([row['label'], word])\n",
    "                              for word in row.words], axis=1)\n",
    "    df_new = pd.DataFrame(rows, columns=[\"label\", \"word\"])\n",
    "    return (\n",
    "        df_new\n",
    "        .groupby(\"word\")\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .sort_values(\"label\", ascending=False)\n",
    "        .head(top)\n",
    "        .word\n",
    "    )\n",
    "\n",
    "\n",
    "def document_features(tweet_words, top_words):\n",
    "    features = {}\n",
    "    # if one of the n_most_common words appears\n",
    "    # in tweet, mark it as \"contains(word)\"\n",
    "    for word in top_words:\n",
    "        features['contains({})'.format(word)] = (word in tweet_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # Use 90% for training, 10% for testing\n",
    "\n",
    "   # Show accuracy of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and Formatting Training Data...\n",
      "Constructing Training Classifier...\n",
      "Results:\n",
      "\n",
      "('Accuracy', 0.731)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # This process takes ~1min and may cause your computer's\n",
    "    # fan to work harder than usual :)\n",
    "\n",
    "\n",
    "    print(\"Reading and Formatting Training Data...\")\n",
    "    tweets = read_train_data()\n",
    "\n",
    "    # only use 10,000 entries for now\n",
    "    # and only look at top 2000 words\n",
    "    print(\"Constructing Training Classifier...\")\n",
    "    top_words = get_top_words(tweets, top=2000)\n",
    "    tweet_words = [(i[1].words, i[1].label)for i in tweets.iterrows()]\n",
    "    features = [(document_features(tweet, top_words), label)\n",
    "                for (tweet, label) in tweet_words][:10000] # only use 10,000 entries for now\n",
    "\n",
    "\n",
    "    # Use 90% for training, 10% for testing\n",
    "    train_set = features[ : int(len(features) * .9) ]\n",
    "    test_set = features[ int(len(features) * .9) : ]\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "\n",
    "    # Show accuracy of trained model\n",
    "    print(\"Results:\\n\")\n",
    "    print(\"Accuracy\", nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can load the Firefox tweets to predict sentiment using\n",
    "   # the above classifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and Formatting Target Data...\n"
     ]
    }
   ],
   "source": [
    "    # Now we can load the Firefox tweets to predict sentiment using\n",
    "    # the above classifier\n",
    "    print(\"Reading and Formatting Target Data...\")\n",
    "    firefox_tweets = read_target_data()\n",
    "    firefox_tweet_words = [i[1].words for i in firefox_tweets.iterrows()]\n",
    "    firefox_features = [document_features(tweet, top_words)\n",
    "       for tweet in firefox_tweet_words]\n",
    "    # assign senitment label to firefox tweets\n",
    "    firefox_tweets.loc[:, 'label'] = classifier.classify_many(firefox_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now predicted sentiment for the Firefox tweets;e can assume this as \"ground truth\" to create a \"dummy classifer\" that allows us to identify words that most associated with negative and positive tweetsmthis is almost an exact copy of the code under the print statement \"Constructing Training Classifier\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing Target (Dummy) Classifier...\n",
      "Most predictive words of sentiment in Firefox Tweets:\n",
      "\n",
      "Most Informative Features\n",
      "     contains(following) = True              pos : neg    =     19.1 : 1.0\n",
      "       contains(upgrade) = True              neg : pos    =     15.9 : 1.0\n",
      "         contains(thank) = True              pos : neg    =     14.0 : 1.0\n",
      "       contains(windows) = True              neg : pos    =     12.6 : 1.0\n",
      "        contains(anyone) = True              neg : pos    =     12.3 : 1.0\n",
      "       contains(history) = True              neg : pos    =     10.7 : 1.0\n",
      "         contains(tried) = True              neg : pos    =      9.6 : 1.0\n",
      "      contains(computer) = True              neg : pos    =      9.6 : 1.0\n",
      "         contains(sorry) = True              neg : pos    =      9.5 : 1.0\n",
      "         contains(check) = True              pos : neg    =      8.3 : 1.0\n",
      "        contains(longer) = True              neg : pos    =      8.1 : 1.0\n",
      "        contains(course) = True              pos : neg    =      8.1 : 1.0\n",
      "         contains(track) = True              pos : neg    =      8.1 : 1.0\n",
      "           contains(pro) = True              neg : pos    =      7.6 : 1.0\n",
      "          contains(crap) = True              neg : pos    =      7.6 : 1.0\n",
      "          contains(hate) = True              neg : pos    =      7.6 : 1.0\n",
      "          contains(year) = True              neg : pos    =      7.4 : 1.0\n",
      "          contains(away) = True              neg : pos    =      7.4 : 1.0\n",
      "        contains(highly) = True              pos : neg    =      7.2 : 1.0\n",
      "           contains(fix) = True              neg : pos    =      7.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "    print(\"Constructing Target (Dummy) Classifier...\") \n",
    "    ff_top_words = get_top_words(firefox_tweets, top=2000)\n",
    "    ff_tweet_words = [(i[1].words, i[1].label)for i in firefox_tweets.iterrows()]\n",
    "    ff_features = [(document_features(tweet, top_words), label)\n",
    "                for (tweet, label) in ff_tweet_words]\n",
    "    ff_classifier = nltk.NaiveBayesClassifier.train(ff_features)\n",
    "\n",
    "\n",
    "    print(\"Most predictive words of sentiment in Firefox Tweets:\\n\")\n",
    "    ff_classifier.show_most_informative_features(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feedback 10/9/18: \n",
    "    Nice!\n",
    "Some nice to have features: \n",
    "   1/  Do we analyze the Support sample - AoA and Help Me only\n",
    "   2/ Week to week change in keywords\n",
    "   3/ analyze the outgoing tweets only (data filtered)\n",
    "   4/analyze the question_answers only, filtered by volunteer *(quality of user reply (are they more positive or negative))\n",
    "        (?character limit?)\n",
    "   5/Train data based on questions_question and questions_answer random sample\n",
    "    b/then - analyze if the positve tweets are more likely solved? \n",
    "    \n",
    "    6/User story: removal of feature in Firefox, are we negative or positive with major product changes in \n",
    "        \n",
    "    Priotize other open text Right away - \n",
    "    Questions in the forum\n",
    "    CSAT\n",
    "    Reddit\n",
    "    google play store (original and responses)\n",
    "    \n",
    "    Action Item: Open git issue\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
